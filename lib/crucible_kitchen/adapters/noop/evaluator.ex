defmodule CrucibleKitchen.Adapters.Noop.Evaluator do
  @moduledoc """
  No-op evaluator adapter for testing and dry runs.

  This adapter returns synthetic evaluation metrics without running
  actual model evaluation. Useful for testing workflows.

  ## Usage

      adapters = %{
        evaluator: CrucibleKitchen.Adapters.Noop.Evaluator,
        ...
      }

      CrucibleKitchen.run(:supervised, config, adapters: adapters, dry_run: true)

  ## Options

  - `:mock_accuracy` - Override mock accuracy (default: random 0.8-0.95)
  - `:mock_f1` - Override mock F1 score (default: random 0.75-0.92)
  - `:mock_delay_ms` - Simulate evaluation delay in ms (default: 0)
  """

  @behaviour CrucibleKitchen.Ports.Evaluator

  require Logger

  @impl true
  def evaluate(opts, _model, dataset) do
    delay_ms = Keyword.get(opts, :mock_delay_ms, 0)
    if delay_ms > 0, do: Process.sleep(delay_ms)

    metrics = Keyword.get(opts, :metrics, [:accuracy, :f1, :precision, :recall])
    sample_count = count_samples(dataset)

    results =
      metrics
      |> Enum.map(fn metric ->
        value = Keyword.get(opts, :"mock_#{metric}") || generate_mock_metric(metric)
        {metric, value}
      end)
      |> Enum.into(%{})
      |> Map.put(:sample_count, sample_count)

    Logger.debug("[Noop.Evaluator] Generated mock results: #{inspect(results)}")
    {:ok, results}
  end

  @impl true
  def generate_report(opts, results) do
    format = Keyword.get(opts, :format, :markdown)

    report =
      case format do
        :markdown -> generate_markdown_report(results)
        :json -> generate_json_report(results)
        _ -> generate_markdown_report(results)
      end

    {:ok, report}
  end

  # Count samples from various dataset formats
  defp count_samples(dataset) when is_list(dataset), do: length(dataset)
  defp count_samples(%{samples: samples}) when is_list(samples), do: length(samples)
  defp count_samples(%{data: data}) when is_list(data), do: length(data)
  defp count_samples(%{test: test}) when is_list(test), do: length(test)
  defp count_samples(_), do: 100

  # Generate realistic mock metrics
  defp generate_mock_metric(:accuracy), do: random_in_range(0.80, 0.95)
  defp generate_mock_metric(:f1), do: random_in_range(0.75, 0.92)
  defp generate_mock_metric(:precision), do: random_in_range(0.78, 0.94)
  defp generate_mock_metric(:recall), do: random_in_range(0.72, 0.91)
  defp generate_mock_metric(:loss), do: random_in_range(0.1, 0.5)
  defp generate_mock_metric(:perplexity), do: random_in_range(1.5, 4.0)
  defp generate_mock_metric(_), do: random_in_range(0.7, 0.9)

  defp random_in_range(min, max) do
    Float.round(min + :rand.uniform() * (max - min), 4)
  end

  # Report generation
  defp generate_markdown_report(results) do
    metrics_section =
      results
      |> Enum.reject(fn {k, _} -> k == :sample_count end)
      |> Enum.sort_by(fn {k, _} -> Atom.to_string(k) end)
      |> Enum.map_join("\n", fn {metric, value} ->
        "| #{format_metric_name(metric)} | #{format_value(value)} |"
      end)

    """
    # Evaluation Report (Mock)

    > Note: This report was generated by the Noop evaluator adapter.

    ## Summary

    - **Samples Evaluated:** #{Map.get(results, :sample_count, 0)}

    ## Metrics

    | Metric | Value |
    |--------|-------|
    #{metrics_section}

    ---
    *Generated by CrucibleKitchen Noop.Evaluator*
    """
  end

  defp generate_json_report(results) do
    Jason.encode!(
      %{
        mock: true,
        results: results
      },
      pretty: true
    )
  end

  defp format_metric_name(metric) do
    metric
    |> Atom.to_string()
    |> String.replace("_", " ")
    |> String.split()
    |> Enum.map_join(" ", &String.capitalize/1)
  end

  defp format_value(value) when is_float(value), do: Float.round(value, 4)
  defp format_value(value), do: value
end
